{"version":3,"file":"log_job.js","sourceRoot":"","sources":["../../../../src/services/logging/log_job.ts"],"names":[],"mappings":";;;AACA,0CAA+C;AAE/C,wCAAqC;AACrC,yBAAuB;AACvB,6CAA0C;AAC1C,mCAAsC;AACtC,IAAA,qBAAY,GAAE,CAAC;AAER,KAAK,UAAU,MAAM,CAAC,GAAiB,EAAE,QAAiB,KAAK;IACpE,IAAI,CAAC;QACH,MAAM,mBAAmB,GAAG,OAAO,CAAC,GAAG,CAAC,qBAAqB,KAAK,MAAM,CAAC;QACzE,IAAI,CAAC,mBAAmB,EAAE,CAAC;YACzB,OAAO;QACT,CAAC;QAED,qDAAqD;QACrD,IACE,GAAG,CAAC,aAAa;YACjB,GAAG,CAAC,aAAa,CAAC,OAAO;YACzB,GAAG,CAAC,aAAa,CAAC,OAAO,CAAC,eAAe,CAAC,EAC1C,CAAC;YACD,GAAG,CAAC,aAAa,CAAC,OAAO,CAAC,eAAe,CAAC,GAAG,UAAU,CAAC;YACxD,GAAG,CAAC,IAAI,GAAG,CAAC,EAAE,OAAO,EAAE,sCAAsC,EAAE,IAAI,EAAE,sCAAsC,EAAE,CAAC,CAAC;QACjH,CAAC;QACD,MAAM,SAAS,GAAG;YAChB,MAAM,EAAE,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,IAAI;YACtC,OAAO,EAAE,GAAG,CAAC,OAAO;YACpB,OAAO,EAAE,GAAG,CAAC,OAAO;YACpB,QAAQ,EAAE,GAAG,CAAC,QAAQ;YACtB,IAAI,EAAE,GAAG,CAAC,IAAI;YACd,UAAU,EAAE,GAAG,CAAC,UAAU;YAC1B,OAAO,EAAE,GAAG,CAAC,OAAO,KAAK,SAAS,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO;YACvD,IAAI,EAAE,GAAG,CAAC,IAAI;YACd,GAAG,EAAE,GAAG,CAAC,GAAG;YACZ,eAAe,EAAE,GAAG,CAAC,cAAc;YACnC,YAAY,EAAE,GAAG,CAAC,aAAa;YAC/B,MAAM,EAAE,GAAG,CAAC,MAAM;YAClB,UAAU,EAAE,GAAG,CAAC,UAAU;YAC1B,KAAK,EAAE,CAAC,CAAC,GAAG,CAAC,KAAK;YAClB,QAAQ,EAAE,GAAG,CAAC,QAAQ;SACvB,CAAC;QAEF,IAAI,KAAK,EAAE,CAAC;YACV,IAAI,CAAC,GAAG,CAAC,EAAE,IAAI,GAAG,KAAK,CAAC;YACxB,OAAO,CAAC,EAAE,IAAI,EAAE,EAAE,CAAC;gBACjB,IAAI,CAAC;oBACH,MAAM,EAAE,KAAK,EAAE,GAAG,MAAM,2BAAgB;yBACrC,IAAI,CAAC,gBAAgB,CAAC;yBACtB,MAAM,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC;oBACvB,IAAI,KAAK,EAAE,CAAC;wBACV,eAAM,CAAC,KAAK,CAAC,yDAAyD,EAAE,EAAE,KAAK,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;wBACzG,MAAM,IAAI,OAAO,CAAO,CAAC,OAAO,EAAE,EAAE,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,OAAO,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;oBACxE,CAAC;yBAAM,CAAC;wBACN,IAAI,GAAG,IAAI,CAAC;wBACZ,MAAM;oBACR,CAAC;gBACH,CAAC;gBAAC,OAAO,KAAK,EAAE,CAAC;oBACf,eAAM,CAAC,KAAK,CAAC,uDAAuD,EAAE,EAAE,KAAK,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;oBACvG,MAAM,IAAI,OAAO,CAAO,CAAC,OAAO,EAAE,EAAE,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,OAAO,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;gBACxE,CAAC;YACH,CAAC;YACD,IAAI,IAAI,EAAE,CAAC;gBACT,eAAM,CAAC,KAAK,CAAC,0BAA0B,EAAE,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;YACrE,CAAC;iBAAM,CAAC;gBACN,eAAM,CAAC,KAAK,CAAC,oBAAoB,EAAE,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;YAC/D,CAAC;QACH,CAAC;aAAM,CAAC;YACN,MAAM,EAAE,KAAK,EAAE,GAAG,MAAM,2BAAgB;iBACrC,IAAI,CAAC,gBAAgB,CAAC;iBACtB,MAAM,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC;YACvB,IAAI,KAAK,EAAE,CAAC;gBACV,eAAM,CAAC,KAAK,CAAC,sBAAsB,KAAK,CAAC,OAAO,EAAE,EAAE,EAAE,KAAK,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;YACvF,CAAC;iBAAM,CAAC;gBACN,eAAM,CAAC,KAAK,CAAC,0BAA0B,EAAE,EAAE,QAAQ,EAAE,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC;YACrE,CAAC;QACH,CAAC;QAED,IAAI,OAAO,CAAC,GAAG,CAAC,eAAe,IAAI,CAAC,GAAG,CAAC,QAAQ,EAAE,CAAC;YACjD,IAAI,KAAK,GAAG;gBACV,UAAU,EAAE,UAAU,EAAE,2NAA2N;gBACnP,GAAG,CAAC,GAAG,CAAC,OAAO,KAAK,SAAS,IAAI;oBAC/B,MAAM,EAAE,EAAE,IAAI,EAAE,GAAG,CAAC,OAAO,EAAE;iBAC9B,CAAC,EAAE,kCAAkC;gBACtC,KAAK,EAAE,YAAY;gBACnB,UAAU,EAAE;oBACV,OAAO,EAAE,GAAG,CAAC,OAAO;oBACpB,OAAO,EAAE,GAAG,CAAC,OAAO;oBACpB,QAAQ,EAAE,GAAG,CAAC,QAAQ;oBACtB,UAAU,EAAE,GAAG,CAAC,UAAU;oBAC1B,OAAO,EAAE,GAAG,CAAC,OAAO,KAAK,SAAS,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO;oBACvD,IAAI,EAAE,GAAG,CAAC,IAAI;oBACd,GAAG,EAAE,GAAG,CAAC,GAAG;oBACZ,eAAe,EAAE,GAAG,CAAC,cAAc;oBACnC,YAAY,EAAE,GAAG,CAAC,aAAa;oBAC/B,MAAM,EAAE,GAAG,CAAC,MAAM;oBAClB,UAAU,EAAE,GAAG,CAAC,UAAU;oBAC1B,KAAK,EAAE,GAAG,CAAC,KAAK;iBACjB;aACF,CAAC;YACF,IAAG,GAAG,CAAC,IAAI,KAAK,aAAa,EAAE,CAAC;gBAC9B,iBAAO,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;YACzB,CAAC;QACH,CAAC;IAEH,CAAC;IAAC,OAAO,KAAK,EAAE,CAAC;QACf,eAAM,CAAC,KAAK,CAAC,sBAAsB,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;IACtD,CAAC;AACH,CAAC;AAnGD,wBAmGC","sourcesContent":["import { ExtractorOptions } from \"./../../lib/entities\";\nimport { supabase_service } from \"../supabase\";\nimport { FirecrawlJob } from \"../../types\";\nimport { posthog } from \"../posthog\";\nimport \"dotenv/config\";\nimport { logger } from \"../../lib/logger\";\nimport { configDotenv } from \"dotenv\";\nconfigDotenv();\n\nexport async function logJob(job: FirecrawlJob, force: boolean = false) {\n  try {\n    const useDbAuthentication = process.env.USE_DB_AUTHENTICATION === 'true';\n    if (!useDbAuthentication) {\n      return;\n    }\n\n    // Redact any pages that have an authorization header\n    if (\n      job.scrapeOptions &&\n      job.scrapeOptions.headers &&\n      job.scrapeOptions.headers[\"Authorization\"]\n    ) {\n      job.scrapeOptions.headers[\"Authorization\"] = \"REDACTED\";\n      job.docs = [{ content: \"REDACTED DUE TO AUTHORIZATION HEADER\", html: \"REDACTED DUE TO AUTHORIZATION HEADER\" }];\n    }\n    const jobColumn = {\n      job_id: job.job_id ? job.job_id : null,\n      success: job.success,\n      message: job.message,\n      num_docs: job.num_docs,\n      docs: job.docs,\n      time_taken: job.time_taken,\n      team_id: job.team_id === \"preview\" ? null : job.team_id,\n      mode: job.mode,\n      url: job.url,\n      crawler_options: job.crawlerOptions,\n      page_options: job.scrapeOptions,\n      origin: job.origin,\n      num_tokens: job.num_tokens,\n      retry: !!job.retry,\n      crawl_id: job.crawl_id,\n    };\n\n    if (force) {\n      let i = 0, done = false;\n      while (i++ <= 10) {\n        try {\n          const { error } = await supabase_service\n            .from(\"firecrawl_jobs\")\n            .insert([jobColumn]);\n          if (error) {\n            logger.error(\"Failed to log job due to Supabase error -- trying again\", { error, scrapeId: job.job_id });\n            await new Promise<void>((resolve) => setTimeout(() => resolve(), 75));\n          } else {\n            done = true;\n            break;\n          }\n        } catch (error) {\n          logger.error(\"Failed to log job due to thrown error -- trying again\", { error, scrapeId: job.job_id });\n          await new Promise<void>((resolve) => setTimeout(() => resolve(), 75));\n        }\n      }\n      if (done) {\n        logger.debug(\"Job logged successfully!\", { scrapeId: job.job_id });\n      } else {\n        logger.error(\"Failed to log job!\", { scrapeId: job.job_id });\n      }\n    } else {\n      const { error } = await supabase_service\n        .from(\"firecrawl_jobs\")\n        .insert([jobColumn]);\n      if (error) {\n        logger.error(`Error logging job: ${error.message}`, { error, scrapeId: job.job_id });\n      } else {\n        logger.debug(\"Job logged successfully!\", { scrapeId: job.job_id });\n      }\n    }\n\n    if (process.env.POSTHOG_API_KEY && !job.crawl_id) {\n      let phLog = {\n        distinctId: \"from-api\", //* To identify this on the group level, setting distinctid to a static string per posthog docs: https://posthog.com/docs/product-analytics/group-analytics#advanced-server-side-only-capturing-group-events-without-a-user\n        ...(job.team_id !== \"preview\" && {\n          groups: { team: job.team_id },\n        }), //* Identifying event on this team\n        event: \"job-logged\",\n        properties: {\n          success: job.success,\n          message: job.message,\n          num_docs: job.num_docs,\n          time_taken: job.time_taken,\n          team_id: job.team_id === \"preview\" ? null : job.team_id,\n          mode: job.mode,\n          url: job.url,\n          crawler_options: job.crawlerOptions,\n          page_options: job.scrapeOptions,\n          origin: job.origin,\n          num_tokens: job.num_tokens,\n          retry: job.retry,\n        },\n      };\n      if(job.mode !== \"single_urls\") {\n        posthog.capture(phLog);\n      }\n    }\n    \n  } catch (error) {\n    logger.error(`Error logging job: ${error.message}`);\n  }\n}\n"]}
{"version":3,"file":"crawler.test.js","sourceRoot":"","sources":["../../../../../src/scraper/WebScraper/__tests__/crawler.test.ts"],"names":[],"mappings":";;;;;AAAA,kBAAkB;AAClB,wCAAwC;AACxC,kDAA0B;AAC1B,kEAAyC;AAEzC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;AACnB,IAAI,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;AAE3B,QAAQ,CAAC,YAAY,EAAE,GAAG,EAAE;IAC1B,IAAI,OAAmB,CAAC;IACxB,MAAM,SAAS,GAAG,eAAkC,CAAC;IACrD,MAAM,gBAAgB,GAAG,uBAAwD,CAAC;IAElF,IAAI,eAAuB,CAAC;IAE5B,UAAU,CAAC,GAAG,EAAE;QACd,sBAAsB;QACtB,SAAS,CAAC,GAAG,CAAC,kBAAkB,CAAC,CAAC,GAAG,EAAE,EAAE;YACvC,IAAI,GAAG,CAAC,QAAQ,CAAC,YAAY,CAAC,EAAE,CAAC;gBAC/B,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,yBAAyB,EAAE,CAAC,CAAC;YAC9D,CAAC;iBAAM,IAAI,GAAG,CAAC,QAAQ,CAAC,aAAa,CAAC,EAAE,CAAC;gBACvC,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,iBAAiB,EAAE,CAAC,CAAC,CAAC,wCAAwC;YAC/F,CAAC;YACD,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,eAAe,EAAE,CAAC,CAAC;QACpD,CAAC,CAAC,CAAC;QAEH,gBAAgB,CAAC,eAAe,CAAC;YAC/B,SAAS,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,IAAI,CAAC;YAC1C,YAAY,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,KAAK,CAAC;YAC9C,qBAAqB,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,CAAC,CAAC;YACnD,aAAa,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,CAAC,CAAC;YAC3C,WAAW,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,EAAE,CAAC;YAC1C,gBAAgB,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,aAAa,CAAC;SAC3D,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,+EAA+E,EAAE,KAAK,IAAI,EAAE;QAC7F,MAAM,UAAU,GAAG,oBAAoB,CAAC;QACxC,MAAM,KAAK,GAAG,CAAC,CAAC,CAAE,sCAAsC;QAExD,OAAO,GAAG,IAAI,oBAAU,CAAC;YACvB,KAAK,EAAE,MAAM;YACb,UAAU,EAAE,UAAU;YACtB,QAAQ,EAAE,EAAE;YACZ,QAAQ,EAAE,EAAE;YACZ,KAAK,EAAE,KAAK,EAAG,kBAAkB;YACjC,eAAe,EAAE,EAAE;SACpB,CAAC,CAAC;QAEH,qEAAqE;QACrE,OAAO,CAAC,sBAAsB,CAAC,GAAG,IAAI,CAAC,EAAE,EAAE,CAAC,iBAAiB,CAAC;YAC5D,UAAU;YACV,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;SACtB,CAAC,CAAC;QAEH,MAAM,aAAa,GAAG,OAAO,CAAC,aAAa,CAAC,CAC1C,CAAC,UAAU,EAAE,UAAU,GAAG,QAAQ,EAAE,UAAU,GAAG,QAAQ,EAAE,UAAU,GAAG,QAAQ,CAAC,EACjF,KAAK,EACL,EAAE,CACH,CAAC;QAEF,MAAM,CAAC,aAAa,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAE,oDAAoD;QAC/F,MAAM,CAAC,aAAa,CAAC,CAAC,OAAO,CAAC;YAC5B,UAAU;YACV,UAAU,GAAG,QAAQ;SACtB,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["// crawler.test.ts\nimport { WebCrawler } from '../crawler';\nimport axios from 'axios';\nimport robotsParser from 'robots-parser';\n\njest.mock('axios');\njest.mock('robots-parser');\n\ndescribe('WebCrawler', () => {\n  let crawler: WebCrawler;\n  const mockAxios = axios as jest.Mocked<typeof axios>;\n  const mockRobotsParser = robotsParser as jest.MockedFunction<typeof robotsParser>;\n\n  let maxCrawledDepth: number;\n\n  beforeEach(() => {\n    // Setup default mocks\n    mockAxios.get.mockImplementation((url) => {\n      if (url.includes('robots.txt')) {\n        return Promise.resolve({ data: 'User-agent: *\\nAllow: /' });\n      } else if (url.includes('sitemap.xml')) {\n        return Promise.resolve({ data: 'sitemap content' }); // You would normally parse this to URLs\n      }\n      return Promise.resolve({ data: '<html></html>' });\n    });\n\n    mockRobotsParser.mockReturnValue({\n      isAllowed: jest.fn().mockReturnValue(true),\n      isDisallowed: jest.fn().mockReturnValue(false),\n      getMatchingLineNumber: jest.fn().mockReturnValue(0),\n      getCrawlDelay: jest.fn().mockReturnValue(0),\n      getSitemaps: jest.fn().mockReturnValue([]),\n      getPreferredHost: jest.fn().mockReturnValue('example.com')\n    });\n  });\n\n  it('should respect the limit parameter by not returning more links than specified', async () => {\n    const initialUrl = 'http://example.com';\n    const limit = 2;  // Set a limit for the number of links\n\n    crawler = new WebCrawler({\n      jobId: \"TEST\",\n      initialUrl: initialUrl,\n      includes: [],\n      excludes: [],\n      limit: limit,  // Apply the limit\n      maxCrawledDepth: 10\n    });\n\n    // Mock sitemap fetching function to return more links than the limit\n    crawler['tryFetchSitemapLinks'] = jest.fn().mockResolvedValue([\n      initialUrl,\n      initialUrl + '/page1',\n      initialUrl + '/page2',\n      initialUrl + '/page3'\n    ]);\n\n    const filteredLinks = crawler['filterLinks'](\n      [initialUrl, initialUrl + '/page1', initialUrl + '/page2', initialUrl + '/page3'],\n      limit,\n      10\n    );\n\n    expect(filteredLinks.length).toBe(limit);  // Check if the number of results respects the limit\n    expect(filteredLinks).toEqual([\n      initialUrl,\n      initialUrl + '/page1'\n    ]);\n  });\n});\n\n"]}
{"version":3,"file":"batch-scrape.js","sourceRoot":"","sources":["../../../../src/controllers/v1/batch-scrape.ts"],"names":[],"mappings":";;;AACA,+BAAoC;AACpC,mCAKiB;AACjB,uDAK+B;AAC/B,gEAA4D;AAE5D,yDAAwD;AACxD,0DAA0D;AAC1D,oDAAqD;AAE9C,KAAK,UAAU,qBAAqB,CACzC,GAA2D,EAC3D,GAA4B;IAE5B,GAAG,CAAC,IAAI,GAAG,gCAAwB,CAAC,KAAK,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;IAEpD,MAAM,EAAE,GAAG,IAAA,SAAM,GAAE,CAAC;IAEpB,MAAM,IAAA,oBAAQ,EAAC,EAAE,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IAErC,IAAI,EAAE,gBAAgB,EAAE,GAAG,GAAG,CAAC,OAAQ,CAAC;IACxC,MAAM,mBAAmB,GAAG,OAAO,CAAC,GAAG,CAAC,qBAAqB,KAAK,MAAM,CAAC;IACzE,IAAG,CAAC,mBAAmB,EAAC,CAAC;QACvB,gBAAgB,GAAG,QAAQ,CAAC;IAC9B,CAAC;IAED,MAAM,EAAE,GAAgB;QACtB,cAAc,EAAE,IAAI;QACpB,aAAa,EAAE,GAAG,CAAC,IAAI;QACvB,eAAe,EAAE,EAAE;QACnB,OAAO,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO;QACzB,SAAS,EAAE,IAAI,CAAC,GAAG,EAAE;QACrB,IAAI,EAAE,GAAG,CAAC,IAAI,CAAC,IAAI;KACpB,CAAC;IAEF,MAAM,IAAA,uBAAS,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;IAExB,IAAI,WAAW,GAAG,EAAE,CAAC;IAErB,uDAAuD;IACvD,kDAAkD;IAClD,IAAG,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,GAAG,IAAI,EAAC,CAAC;QAC9B,iBAAiB;QACjB,WAAW,GAAG,MAAM,IAAA,6BAAc,EAAC,EAAC,IAAI,EAAE,GAAG,CAAC,IAAI,CAAC,IAAI,EAAE,OAAO,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO,EAAE,YAAY,EAAE,EAAE,EAAC,CAAC,CAAA;IACxG,CAAC;IAED,MAAM,IAAI,GAAG,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE;QACnC,OAAO;YACL,IAAI,EAAE;gBACJ,GAAG,EAAE,CAAC;gBACN,IAAI,EAAE,aAAsB;gBAC5B,OAAO,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO;gBACzB,IAAI,EAAE,GAAG,CAAC,IAAI,CAAC,IAAK;gBACpB,cAAc,EAAE,IAAI;gBACpB,aAAa,EAAE,GAAG,CAAC,IAAI;gBACvB,MAAM,EAAE,KAAK;gBACb,QAAQ,EAAE,EAAE;gBACZ,UAAU,EAAE,IAAI;gBAChB,EAAE,EAAE,IAAI;gBACR,OAAO,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO;aAC1B;YACD,IAAI,EAAE;gBACJ,KAAK,EAAE,IAAA,SAAM,GAAE;gBACf,QAAQ,EAAE,EAAE;aACb;SACF,CAAC;IACJ,CAAC,CAAC,CAAC;IAEH,MAAM,IAAA,sBAAQ,EACZ,EAAE,EACF,EAAE,EACF,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,CAC5B,CAAC;IACF,MAAM,IAAA,0BAAY,EAChB,EAAE,EACF,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAC9B,CAAC;IACF,MAAM,IAAA,0BAAa,EAAC,IAAI,CAAC,CAAC;IAE1B,IAAG,GAAG,CAAC,IAAI,CAAC,OAAO,EAAE,CAAC;QACpB,MAAM,IAAA,qBAAW,EAAC,GAAG,CAAC,IAAI,CAAC,OAAO,EAAE,EAAE,EAAE,IAAI,EAAE,GAAG,CAAC,IAAI,CAAC,OAAO,EAAE,IAAI,EAAE,sBAAsB,CAAC,CAAC;IAChG,CAAC;IAED,MAAM,QAAQ,GAAG,OAAO,CAAC,GAAG,CAAC,GAAG,KAAK,OAAO,CAAC,CAAC,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC,CAAC,OAAO,CAAC;IAEtE,OAAO,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC;QAC1B,OAAO,EAAE,IAAI;QACb,EAAE;QACF,GAAG,EAAE,GAAG,QAAQ,MAAM,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,oBAAoB,EAAE,EAAE;KAC9D,CAAC,CAAC;AACL,CAAC;AAhFD,sDAgFC","sourcesContent":["import { Response } from \"express\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\n  BatchScrapeRequest,\n  batchScrapeRequestSchema,\n  CrawlResponse,\n  RequestWithAuth,\n} from \"./types\";\nimport {\n  addCrawlJobs,\n  lockURLs,\n  saveCrawl,\n  StoredCrawl,\n} from \"../../lib/crawl-redis\";\nimport { logCrawl } from \"../../services/logging/crawl_log\";\nimport { getScrapeQueue } from \"../../services/queue-service\";\nimport { getJobPriority } from \"../../lib/job-priority\";\nimport { addScrapeJobs } from \"../../services/queue-jobs\";\nimport { callWebhook } from \"../../services/webhook\";\n\nexport async function batchScrapeController(\n  req: RequestWithAuth<{}, CrawlResponse, BatchScrapeRequest>,\n  res: Response<CrawlResponse>\n) {\n  req.body = batchScrapeRequestSchema.parse(req.body);\n\n  const id = uuidv4();\n\n  await logCrawl(id, req.auth.team_id);\n\n  let { remainingCredits } = req.account!;\n  const useDbAuthentication = process.env.USE_DB_AUTHENTICATION === 'true';\n  if(!useDbAuthentication){\n    remainingCredits = Infinity;\n  }\n\n  const sc: StoredCrawl = {\n    crawlerOptions: null,\n    scrapeOptions: req.body,\n    internalOptions: {},\n    team_id: req.auth.team_id,\n    createdAt: Date.now(),\n    plan: req.auth.plan,\n  };\n\n  await saveCrawl(id, sc);\n\n  let jobPriority = 20;\n\n  // If it is over 1000, we need to get the job priority,\n  // otherwise we can use the default priority of 20\n  if(req.body.urls.length > 1000){\n    // set base to 21\n    jobPriority = await getJobPriority({plan: req.auth.plan, team_id: req.auth.team_id, basePriority: 21})\n  }\n\n  const jobs = req.body.urls.map((x) => {\n    return {\n      data: {\n        url: x,\n        mode: \"single_urls\" as const,\n        team_id: req.auth.team_id,\n        plan: req.auth.plan!,\n        crawlerOptions: null,\n        scrapeOptions: req.body,\n        origin: \"api\",\n        crawl_id: id,\n        sitemapped: true,\n        v1: true,\n        webhook: req.body.webhook,\n      },\n      opts: {\n        jobId: uuidv4(),\n        priority: 20,\n      },\n    };\n  });\n\n  await lockURLs(\n    id,\n    sc,\n    jobs.map((x) => x.data.url)\n  );\n  await addCrawlJobs(\n    id,\n    jobs.map((x) => x.opts.jobId)\n  );\n  await addScrapeJobs(jobs);\n\n  if(req.body.webhook) {\n    await callWebhook(req.auth.team_id, id, null, req.body.webhook, true, \"batch_scrape.started\");\n  }\n\n  const protocol = process.env.ENV === \"local\" ? req.protocol : \"https\";\n  \n  return res.status(200).json({\n    success: true,\n    id,\n    url: `${protocol}://${req.get(\"host\")}/v1/batch/scrape/${id}`,\n  });\n}\n\n\n"]}
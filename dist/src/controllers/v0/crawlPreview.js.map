{"version":3,"file":"crawlPreview.js","sourceRoot":"","sources":["../../../../src/controllers/v0/crawlPreview.ts"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;AACA,kCAA2C;AAC3C,8CAAqD;AACrD,+EAA+E;AAC/E,+BAAoC;AACpC,oDAAiD;AACjD,8DAA4G;AAC5G,iEAAgE;AAChE,8DAAiE;AACjE,qDAAuC;AACvC,uCAAsD;AAE/C,KAAK,UAAU,sBAAsB,CAAC,GAAY,EAAE,GAAa;IACtE,IAAI,CAAC;QACH,MAAM,IAAI,GAAG,MAAM,IAAA,uBAAgB,EACjC,GAAG,EACH,GAAG,EACH,uBAAe,CAAC,OAAO,CACxB,CAAC;QAEF,MAAM,OAAO,GAAG,SAAS,CAAC;QAE1B,IAAI,CAAC,IAAI,CAAC,OAAO,EAAE,CAAC;YAClB,OAAO,GAAG,CAAC,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,KAAK,EAAE,CAAC,CAAC;QAC7D,CAAC;QAED,MAAM,EAAE,IAAI,EAAE,GAAG,IAAI,CAAC;QAEtB,IAAI,GAAG,GAAG,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC;QACvB,IAAI,CAAC,GAAG,EAAE,CAAC;YACT,OAAO,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,EAAE,KAAK,EAAE,iBAAiB,EAAE,CAAC,CAAC;QAC5D,CAAC;QACD,IAAI,CAAC;YACH,GAAG,GAAG,IAAA,+BAAiB,EAAC,GAAG,CAAC,CAAC,GAAG,CAAC;QACnC,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,OAAO,GAAG;iBACP,MAAM,CAAC,CAAC,YAAY,KAAK,IAAI,CAAC,CAAC,OAAO,KAAK,aAAa,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC;iBACrE,IAAI,CAAC,EAAE,KAAK,EAAE,CAAC,CAAC,OAAO,IAAI,CAAC,EAAE,CAAC,CAAC;QACrC,CAAC;QAED,IAAI,IAAA,wBAAY,EAAC,GAAG,CAAC,EAAE,CAAC;YACtB,OAAO,GAAG;iBACP,MAAM,CAAC,GAAG,CAAC;iBACX,IAAI,CAAC;gBACJ,KAAK,EACH,2IAA2I;aAC9I,CAAC,CAAC;QACP,CAAC;QAED,MAAM,cAAc,GAAG,GAAG,CAAC,IAAI,CAAC,cAAc,IAAI,EAAE,CAAC;QACrD,MAAM,WAAW,GAAG,GAAG,CAAC,IAAI,CAAC,WAAW,IAAI,EAAE,eAAe,EAAE,KAAK,EAAE,WAAW,EAAE,KAAK,EAAE,UAAU,EAAE,EAAE,EAAE,CAAC;QAE3G,gFAAgF;QAChF,UAAU;QACV,8CAA8C;QAC9C,2BAA2B;QAC3B,yBAAyB;QACzB,6BAA6B;QAC7B,qBAAqB;QACrB,qEAAqE;QACrE,kCAAkC;QAClC,UAAU;QAEV,+DAA+D;QAC/D,6BAA6B;QAC7B,qCAAqC;QACrC,iCAAiC;QACjC,oCAAoC;QACpC,oDAAoD;QACpD,YAAY;QACZ,UAAU;QACV,wBAAwB;QACxB,uBAAuB;QACvB,yBAAyB;QACzB,UAAU;QACV,sBAAsB;QACtB,2BAA2B;QAC3B,6DAA6D;QAC7D,MAAM;QACN,IAAI;QAEJ,MAAM,EAAE,GAAG,IAAA,SAAM,GAAE,CAAC;QAEpB,IAAI,MAAM,CAAC;QAEX,IAAI,CAAC;YACH,MAAM,GAAG,MAAM,IAAI,CAAC,YAAY,EAAE,CAAC;QACrC,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC,CAAA,CAAC;QAEd,MAAM,EAAE,aAAa,EAAE,eAAe,EAAE,GAAG,IAAA,+BAAuB,EAAC,WAAW,EAAE,SAAS,EAAE,SAAS,CAAC,CAAC;QAEtG,MAAM,EAAE,GAAgB;YACtB,SAAS,EAAE,GAAG;YACd,cAAc;YACd,aAAa;YACb,eAAe;YACf,OAAO;YACP,IAAI;YACJ,MAAM;YACN,SAAS,EAAE,IAAI,CAAC,GAAG,EAAE;SACtB,CAAC;QAEF,MAAM,IAAA,uBAAS,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;QAExB,MAAM,OAAO,GAAG,IAAA,4BAAc,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;QAEvC,MAAM,OAAO,GAAG,EAAE,CAAC,cAAc,EAAE,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,OAAO,CAAC,aAAa,EAAE,CAAC;QAExF,IAAI,OAAO,KAAK,IAAI,EAAE,CAAC;YACrB,KAAK,MAAM,GAAG,IAAI,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC;gBAC1C,MAAM,IAAA,qBAAO,EAAC,EAAE,EAAE,EAAE,EAAE,GAAG,CAAC,CAAC;gBAC3B,MAAM,KAAK,GAAG,IAAA,SAAM,GAAE,CAAC;gBACvB,MAAM,IAAA,yBAAY,EAAC;oBACjB,GAAG;oBACH,IAAI,EAAE,aAAa;oBACnB,OAAO;oBACP,IAAI,EAAE,IAAK;oBACX,cAAc;oBACd,aAAa;oBACb,eAAe;oBACf,MAAM,EAAE,iBAAiB;oBACzB,QAAQ,EAAE,EAAE;oBACZ,UAAU,EAAE,IAAI;iBACjB,EAAE,EAAE,EAAE,KAAK,CAAC,CAAC;gBACd,MAAM,IAAA,yBAAW,EAAC,EAAE,EAAE,KAAK,CAAC,CAAC;YAC/B,CAAC;QACH,CAAC;aAAM,CAAC;YACN,MAAM,IAAA,qBAAO,EAAC,EAAE,EAAE,EAAE,EAAE,GAAG,CAAC,CAAC;YAC3B,MAAM,KAAK,GAAG,IAAA,SAAM,GAAE,CAAC;YACvB,MAAM,IAAA,yBAAY,EAAC;gBACjB,GAAG;gBACH,IAAI,EAAE,aAAa;gBACnB,OAAO;gBACP,IAAI,EAAE,IAAK;gBACX,cAAc;gBACd,aAAa;gBACb,eAAe;gBACf,MAAM,EAAE,iBAAiB;gBACzB,QAAQ,EAAE,EAAE;aACb,EAAE,EAAE,EAAE,KAAK,CAAC,CAAC;YACd,MAAM,IAAA,yBAAW,EAAC,EAAE,EAAE,KAAK,CAAC,CAAC;QAC/B,CAAC;QAED,GAAG,CAAC,IAAI,CAAC,EAAE,KAAK,EAAE,EAAE,EAAE,CAAC,CAAC;IAC1B,CAAC;IAAC,OAAO,KAAK,EAAE,CAAC;QACf,MAAM,CAAC,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAC/B,eAAM,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC;QACpB,OAAO,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,EAAE,KAAK,EAAE,KAAK,CAAC,OAAO,EAAE,CAAC,CAAC;IACxD,CAAC;AACH,CAAC;AAzID,wDAyIC","sourcesContent":["import { Request, Response } from \"express\";\nimport { authenticateUser } from \"../auth\";\nimport { RateLimiterMode } from \"../../../src/types\";\nimport { isUrlBlocked } from \"../../../src/scraper/WebScraper/utils/blocklist\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { logger } from \"../../../src/lib/logger\";\nimport { addCrawlJob, crawlToCrawler, lockURL, saveCrawl, StoredCrawl } from \"../../../src/lib/crawl-redis\";\nimport { addScrapeJob } from \"../../../src/services/queue-jobs\";\nimport { checkAndUpdateURL } from \"../../../src/lib/validateUrl\";\nimport * as Sentry from \"@sentry/node\";\nimport { fromLegacyScrapeOptions } from \"../v1/types\";\n\nexport async function crawlPreviewController(req: Request, res: Response) {\n  try {\n    const auth = await authenticateUser(\n      req,\n      res,\n      RateLimiterMode.Preview\n    );\n\n    const team_id = \"preview\";\n\n    if (!auth.success) {\n      return res.status(auth.status).json({ error: auth.error });\n    }\n\n    const { plan } = auth;\n\n    let url = req.body.url;\n    if (!url) {\n      return res.status(400).json({ error: \"Url is required\" });\n    }\n    try {\n      url = checkAndUpdateURL(url).url;\n    } catch (e) {\n      return res\n        .status(e instanceof Error && e.message === \"Invalid URL\" ? 400 : 500)\n        .json({ error: e.message ?? e });\n    }\n\n    if (isUrlBlocked(url)) {\n      return res\n        .status(403)\n        .json({\n          error:\n            \"Firecrawl currently does not support social media scraping due to policy restrictions. We're actively working on building support for it.\",\n        });\n    }\n\n    const crawlerOptions = req.body.crawlerOptions ?? {};\n    const pageOptions = req.body.pageOptions ?? { onlyMainContent: false, includeHtml: false, removeTags: [] };\n\n    // if (mode === \"single_urls\" && !url.includes(\",\")) { // NOTE: do we need this?\n    //   try {\n    //     const a = new WebScraperDataProvider();\n    //     await a.setOptions({\n    //       jobId: uuidv4(),\n    //       mode: \"single_urls\",\n    //       urls: [url],\n    //       crawlerOptions: { ...crawlerOptions, returnOnlyUrls: true },\n    //       pageOptions: pageOptions,\n    //     });\n\n    //     const docs = await a.getDocuments(false, (progress) => {\n    //       job.updateProgress({\n    //         current: progress.current,\n    //         total: progress.total,\n    //         current_step: \"SCRAPING\",\n    //         current_url: progress.currentDocumentUrl,\n    //       });\n    //     });\n    //     return res.json({\n    //       success: true,\n    //       documents: docs,\n    //     });\n    //   } catch (error) {\n    //     logger.error(error);\n    //     return res.status(500).json({ error: error.message });\n    //   }\n    // }\n\n    const id = uuidv4();\n\n    let robots;\n\n    try {\n      robots = await this.getRobotsTxt();\n    } catch (_) {}\n\n    const { scrapeOptions, internalOptions } = fromLegacyScrapeOptions(pageOptions, undefined, undefined);\n\n    const sc: StoredCrawl = {\n      originUrl: url,\n      crawlerOptions,\n      scrapeOptions,\n      internalOptions,\n      team_id,\n      plan,\n      robots,\n      createdAt: Date.now(),\n    };\n\n    await saveCrawl(id, sc);\n\n    const crawler = crawlToCrawler(id, sc);\n\n    const sitemap = sc.crawlerOptions?.ignoreSitemap ? null : await crawler.tryGetSitemap();\n\n    if (sitemap !== null) {\n      for (const url of sitemap.map(x => x.url)) {\n        await lockURL(id, sc, url);\n        const jobId = uuidv4();\n        await addScrapeJob({\n          url,\n          mode: \"single_urls\",\n          team_id,\n          plan: plan!,\n          crawlerOptions,\n          scrapeOptions,\n          internalOptions,\n          origin: \"website-preview\",\n          crawl_id: id,\n          sitemapped: true,\n        }, {}, jobId);\n        await addCrawlJob(id, jobId);\n      }\n    } else {\n      await lockURL(id, sc, url);\n      const jobId = uuidv4();\n      await addScrapeJob({\n        url,\n        mode: \"single_urls\",\n        team_id,\n        plan: plan!,\n        crawlerOptions,\n        scrapeOptions,\n        internalOptions,\n        origin: \"website-preview\",\n        crawl_id: id,\n      }, {}, jobId);\n      await addCrawlJob(id, jobId);\n    }\n\n    res.json({ jobId: id });\n  } catch (error) {\n    Sentry.captureException(error);\n    logger.error(error);\n    return res.status(500).json({ error: error.message });\n  }\n}\n"]}